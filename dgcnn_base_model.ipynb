{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71tVeiGBRziZ"
      },
      "source": [
        "### 1. Dynamic Graph CNN base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMa5v62-Rzic"
      },
      "source": [
        "##### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HttGoUfWlOqy",
        "outputId": "84d98be5-6866-4a50-c80c-446f1fd83f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m1XeysaJlJKN",
        "outputId": "71be66c4-268f-49c1-d17f-ce97b67b58e7"
      },
      "outputs": [],
      "source": [
        "!pip install unrar\n",
        "!unrar x /content/drive/MyDrive/Disertatie/data_slim.rar\n",
        "!unrar x /content/drive/MyDrive/Disertatie/metadata.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwUYfSd_m_rO",
        "outputId": "0541c40f-00d2-44db-849d-c20166ae96e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting plyfile\n",
            "  Downloading plyfile-1.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile) (1.26.4)\n",
            "Downloading plyfile-1.0.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: plyfile\n",
            "Successfully installed plyfile-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install plyfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3fGmKP5cRzid"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import glob\n",
        "import h5py\n",
        "import json\n",
        "import cv2\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import urllib.request as urllib\n",
        "import zipfile\n",
        "import os\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.metrics as metrics\n",
        "from plyfile import PlyData, PlyElement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHyTXMU9Rzie"
      },
      "source": [
        "##### Utility functions, such as loss etc. & Global vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1vWzX7pZRzif"
      },
      "outputs": [],
      "source": [
        "def knn(x, k):\n",
        "    inner_product = -2*torch.matmul(x.transpose(2, 1), x)\n",
        "    squared_norms = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = -squared_norms - inner_product - squared_norms.transpose(2, 1)\n",
        "\n",
        "    # (batch_size, num_points, k)\n",
        "    idx = pairwise_distance.topk(k=k, dim=-1)[1]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def cal_loss(pred, gold, smoothing=False, ignore_index=255):\n",
        "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
        "\n",
        "    gold = gold.contiguous().view(-1)\n",
        "\n",
        "    if smoothing:\n",
        "        eps = 0.2\n",
        "        n_class = pred.size(1)\n",
        "\n",
        "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "        log_prb = F.log_softmax(pred, dim=1)\n",
        "\n",
        "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
        "    else:\n",
        "        loss = F.cross_entropy(\n",
        "            pred, gold, reduction='mean',\n",
        "            ignore_index=ignore_index)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_graph_feature(x, k=20, idx=None, dim9=False):\n",
        "    batch_size = x.size(0)\n",
        "    num_points = x.size(2)\n",
        "    x = x.view(batch_size, -1, num_points)\n",
        "    if idx is None:\n",
        "        if dim9 == False:\n",
        "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
        "        else:\n",
        "            idx = knn(x[:, 6:], k=k)\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    idx_base = torch.arange(\n",
        "        0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
        "\n",
        "    idx = idx + idx_base\n",
        "\n",
        "    idx = idx.view(-1)\n",
        "\n",
        "    _, num_dims, _ = x.size()\n",
        "\n",
        "    # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
        "    x = x.transpose(2, 1).contiguous()\n",
        "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
        "    feature = feature.view(batch_size, num_points, k, num_dims)\n",
        "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
        "\n",
        "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "    return feature\n",
        "\n",
        "\n",
        "def calculate_semantic_mIoU(pred_np, true_np, visual=False):\n",
        "    I_all = np.zeros(13)\n",
        "    U_all = np.zeros(13)\n",
        "    for sem_idx in range(true_np.shape[0]):\n",
        "        for sem in range(13):\n",
        "            I = np.sum(np.logical_and(\n",
        "                pred_np[sem_idx] == sem, true_np[sem_idx] == sem))\n",
        "            U = np.sum(np.logical_or(\n",
        "                pred_np[sem_idx] == sem, true_np[sem_idx] == sem))\n",
        "            I_all[sem] += I\n",
        "            U_all[sem] += U\n",
        "    return I_all / U_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK43Z6sSRzif"
      },
      "source": [
        "##### Experimental Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JL2WHXVJRzig"
      },
      "outputs": [],
      "source": [
        "def adaptive_knn_local_density(x, base_k=5, density_scale=1.0):\n",
        "    \"\"\"\n",
        "    Compute k-nearest neighbors with adaptive neighborhood sizes based on local density.\n",
        "\n",
        "    Args:\n",
        "    - x (Tensor): The input tensor of shape (batch_size, num_dims, num_points).\n",
        "    - base_k (int): The base number of neighbors used to estimate density.\n",
        "    - density_scale (float): Scaling factor to adjust k based on density.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Indices of the k-nearest neighbors, with adaptive k for each point.\n",
        "    \"\"\"\n",
        "    batch_size, num_dims, num_points = x.size()\n",
        "\n",
        "    # Step 1: Calculate initial distances for density estimation using a fixed small k\n",
        "    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = xx - inner + xx.transpose(2, 1)\n",
        "\n",
        "    # Get initial k neighbors to estimate density\n",
        "    initial_k = min(base_k, num_points - 1)\n",
        "    initial_idx = pairwise_distance.topk(k=initial_k, largest=False, dim=-1)[1]\n",
        "\n",
        "    # Step 2: Estimate local density\n",
        "    # Using mean distance to initial k neighbors as a simple density measure\n",
        "    density = torch.mean(torch.gather(\n",
        "        pairwise_distance, 2, initial_idx), dim=-1)\n",
        "    # average density per batch\n",
        "    avg_density = torch.mean(density, dim=-1, keepdim=True)\n",
        "    relative_density = density*density_scale / avg_density\n",
        "\n",
        "    # Step 3: Determine adaptive k for each point\n",
        "    adaptive_k = torch.clamp(\n",
        "        (base_k * torch.reciprocal(relative_density)).long(), min=2, max=num_points - 1)\n",
        "\n",
        "    # Step 4: Compute KNN using adaptive k for each point\n",
        "    idx_adaptive = torch.zeros(batch_size, num_points, torch.max(\n",
        "        adaptive_k).item(), dtype=torch.long, device=x.device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(num_points):\n",
        "            k = adaptive_k[i, j].item()\n",
        "            distances = pairwise_distance[i, j, :]\n",
        "            _, indices = torch.topk(distances, k=k, largest=False)\n",
        "            idx_adaptive[i, j, :k] = indices\n",
        "\n",
        "    return idx_adaptive\n",
        "\n",
        "\n",
        "def calculate_ktest(k_train, bias, n_train, n_test):\n",
        "    \"\"\" Calculate the adjusted K for testing based on training parameters. \"\"\"\n",
        "    return int((k_train - bias) * (n_test / n_train) + bias)\n",
        "\n",
        "\n",
        "def adaptive_knn(x, n_train, k_train=20, bias=5):\n",
        "    \"\"\"\n",
        "    Compute k-nearest neighbors with dynamically adjusted neighborhood sizes based on input size.\n",
        "\n",
        "    Args:\n",
        "    - x (Tensor): The input tensor of shape (batch_size, num_dims, num_points).\n",
        "    - n_train (int): The number of points in the training dataset.\n",
        "    - k_train (int): The base number of neighbors used during training.\n",
        "    - bias (int): The smallest size of the KNN model.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: Indices of the k-nearest neighbors, with dynamically adjusted k.\n",
        "    \"\"\"\n",
        "    batch_size, num_dims, n_test = x.size()\n",
        "    k_test = calculate_ktest(k_train, bias, n_train, n_test)\n",
        "\n",
        "    # Calculate pairwise squared Euclidean distance\n",
        "    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
        "    pairwise_distance = xx - inner + xx.transpose(2, 1)\n",
        "\n",
        "    # Get the top k_test neighbors\n",
        "    # (batch_size, num_points, k_test)\n",
        "    idx = pairwise_distance.topk(k=k_test, largest=False, dim=-1)[1]\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuhPYHc_Rzig"
      },
      "source": [
        "##### Model Implementation for Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_1Q_6FFRzig"
      },
      "source": [
        "Starting with <strong>PointNet</strong> implementation, as it will be our backbone, on top of which we will add the EdgeConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KKDfAUqwRzih"
      },
      "outputs": [],
      "source": [
        "class PointNet(nn.Module):\n",
        "    def __init__(self, args, out_channels=40):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv_1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n",
        "        self.conv_2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
        "        self.conv_3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
        "        self.conv_4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n",
        "        self.conv_5 = nn.Conv1d(128, args['embedded_dims'], kernel_size=1, bias=False)\n",
        "        self.batch_norm_1 = nn.BatchNorm1d(64)\n",
        "        self.batch_norm_2 = nn.BatchNorm1d(64)\n",
        "        self.batch_norm_3 = nn.BatchNorm1d(64)\n",
        "        self.batch_norm_4 = nn.BatchNorm1d(128)\n",
        "        self.batch_norm_5 = nn.BatchNorm1d(args['embedded_dims'])\n",
        "        self.fc_1 = nn.Linear(args['embedded_dims'], 512, bias=False)\n",
        "        self.batch_norm_6 = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.fc_2 = nn.Linear(512, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.batch_norm_1(self.conv_1(x)))\n",
        "        x = F.relu(self.batch_norm_2(self.conv_2(x)))\n",
        "        x = F.relu(self.batch_norm_3(self.conv_3(x)))\n",
        "        x = F.relu(self.batch_norm_4(self.conv_4(x)))\n",
        "        x = F.relu(self.batch_norm_5(self.conv_5(x)))\n",
        "        x = F.adaptive_max_pool1d(x, 1).squeeze()\n",
        "        x = F.relu(self.batch_norm_6(self.fc_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw-5qVk0Rzih"
      },
      "source": [
        "Next, we implement the <strong>EdgeConv</strong> network, specifically tailored for semantic segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "upzcN0AkRzih"
      },
      "outputs": [],
      "source": [
        "class DGCNN(nn.Module):\n",
        "    def __init__(self, args, num_clases=13):\n",
        "        super(DGCNN, self).__init__()\n",
        "        self.args = args\n",
        "        self.k = args['k']\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_5 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm_6 = nn.BatchNorm1d(args['embedded_dims'])\n",
        "        self.batch_norm_7 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm_8 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.conv_1 = nn.Sequential(nn.Conv2d(18, 64, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_1,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_2,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_3,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_4,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_5,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_6 = nn.Sequential(nn.Conv1d(192, args['embedded_dims'], kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_6,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_7 = nn.Sequential(nn.Conv1d(1216, 512, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_7,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.conv_8 = nn.Sequential(nn.Conv1d(512, 256, kernel_size=1, bias=False),\n",
        "                                    self.batch_norm_8,\n",
        "                                    nn.LeakyReLU(negative_slope=0.2))\n",
        "        self.dropout = nn.Dropout(p=args['dropout'])\n",
        "        self.conv_9 = nn.Conv1d(256, num_clases, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        num_points = x.size(2)\n",
        "\n",
        "        x = get_graph_feature(x, k=self.k, dim9=True)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k)\n",
        "        x = self.conv_1(x)                              # (batch_size, 9*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x = self.conv_2(x)                              # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x1 = x.max(dim=-1, keepdim=False)[0]            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
        "\n",
        "        x = get_graph_feature(x1, k=self.k)             # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
        "        x = self.conv_3(x)                              # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x = self.conv_4(x)                              # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x2 = x.max(dim=-1, keepdim=False)[0]            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
        "\n",
        "        x = get_graph_feature(x2, k=self.k)             # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
        "        x = self.conv_5(x)                              # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
        "        x3 = x.max(dim=-1, keepdim=False)[0]            # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
        "\n",
        "        x = torch.cat((x1, x2, x3), dim=1)              # (batch_size, 64*3, num_points)\n",
        "\n",
        "        x = self.conv_6(x)                              # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n",
        "        x = x.max(dim=-1, keepdim=True)[0]              # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims, 1)\n",
        "\n",
        "        x = x.repeat(1, 1, num_points)                  # (batch_size, 1024, num_points)\n",
        "        x = torch.cat((x, x1, x2, x3), dim=1)           # (batch_size, 1024+64*3, num_points)\n",
        "\n",
        "        x = self.conv_7(x)                              # (batch_size, 1024+64*3, num_points) -> (batch_size, 512, num_points)\n",
        "        x = self.conv_8(x)                              # (batch_size, 512, num_points) -> (batch_size, 256, num_points)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv_9(x)                              # (batch_size, 256, num_points) -> (batch_size, 13, num_points)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGwT3lwmRzii"
      },
      "source": [
        "### 2. Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_3vfGWQRzii"
      },
      "source": [
        "##### Some constant vars for the s3dis dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M94ypv1ARzii"
      },
      "outputs": [],
      "source": [
        "g_classes = [x.rstrip() for x in open('metadata/class_names.txt')]\n",
        "g_class2label = {cls: i for i, cls in enumerate(g_classes)}\n",
        "g_class2color = {'ceiling':\t[0, 255, 0],\n",
        "                 'floor':\t[0, 0, 255],\n",
        "                 'wall':\t[0, 255, 255],\n",
        "                 'beam':    [255, 255, 0],\n",
        "                 'column':  [255, 0, 255],\n",
        "                 'window':  [100, 100, 255],\n",
        "                 'door':    [200, 200, 100],\n",
        "                 'table':   [170, 120, 200],\n",
        "                 'chair':   [255, 0, 0],\n",
        "                 'sofa':    [200, 100, 100],\n",
        "                 'bookcase':[10, 200, 100],\n",
        "                 'board':   [200, 200, 200],\n",
        "                 'clutter': [50, 50, 50]}\n",
        "g_easy_view_labels = [7, 8, 9, 10, 11, 1]\n",
        "g_label2color = {g_classes.index(cls): g_class2color[cls] for cls in g_classes}\n",
        "\n",
        "global raw_data_index\n",
        "raw_data_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REbkN3cIRzij"
      },
      "source": [
        "##### Utils for data handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6ZO-TQpURzij"
      },
      "outputs": [],
      "source": [
        "def sample_data(data, num_sample):\n",
        "    \"\"\" data is in N x ...\n",
        "        we want to keep num_samplexC of them.\n",
        "        if N > num_sample, we will randomly keep num_sample of them.\n",
        "        if N < num_sample, we will randomly duplicate samples.\n",
        "    \"\"\"\n",
        "    N = data.shape[0]\n",
        "    if (N == num_sample):\n",
        "        return data, range(N)\n",
        "    elif (N > num_sample):\n",
        "        sample = np.random.choice(N, num_sample)\n",
        "        return data[sample, ...], sample\n",
        "    else:\n",
        "        sample = np.random.choice(N, num_sample-N)\n",
        "        dup_data = data[sample, ...]\n",
        "        return np.concatenate([data, dup_data], 0), list(range(N))+list(sample)\n",
        "\n",
        "def sample_data_label(data, label, num_sample):\n",
        "    new_data, sample_indices = sample_data(data, num_sample)\n",
        "    new_label = label[sample_indices]\n",
        "    return new_data, new_label\n",
        "\n",
        "def room2blocks(data_label_filename, data, label, num_point, block_size=1.0, stride=1.0,\n",
        "                random_sample=False, sample_num=None, sample_aug=1):\n",
        "    \"\"\" Prepare block training data.\n",
        "    Args:\n",
        "        data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n",
        "            assumes the data is shifted (min point is origin) and aligned\n",
        "            (aligned with XYZ axis)\n",
        "        label: N size uint8 numpy array from 0-12\n",
        "        num_point: int, how many points to sample in each block\n",
        "        block_size: float, physical size of the block in meters\n",
        "        stride: float, stride for block sweeping\n",
        "        random_sample: bool, if True, we will randomly sample blocks in the room\n",
        "        sample_num: int, if random sample, how many blocks to sample\n",
        "            [default: room area]\n",
        "        sample_aug: if random sample, how much aug\n",
        "    Returns:\n",
        "        block_datas: K x num_point x 6 np array of XYZRGB, RGB is in [0,1]\n",
        "        block_labels: K x num_point x 1 np array of uint8 labels\n",
        "    \"\"\"\n",
        "    assert (stride <= block_size)\n",
        "    print(data_label_filename)\n",
        "    limit = np.amax(data, 0)[0:3]\n",
        "\n",
        "    # Get the corner location for our sampling blocks\n",
        "    xbeg_list = []\n",
        "    ybeg_list = []\n",
        "    if not random_sample:\n",
        "        num_block_x = int(np.ceil((limit[0] - block_size) / stride)) + 1\n",
        "        num_block_y = int(np.ceil((limit[1] - block_size) / stride)) + 1\n",
        "        for i in range(num_block_x):\n",
        "            for j in range(num_block_y):\n",
        "                xbeg_list.append(i*stride)\n",
        "                ybeg_list.append(j*stride)\n",
        "    else:\n",
        "        num_block_x = int(np.ceil(limit[0] / block_size))\n",
        "        num_block_y = int(np.ceil(limit[1] / block_size))\n",
        "        if sample_num is None:\n",
        "            sample_num = num_block_x * num_block_y * sample_aug\n",
        "        for _ in range(sample_num):\n",
        "            xbeg = np.random.uniform(-block_size, limit[0])\n",
        "            ybeg = np.random.uniform(-block_size, limit[1])\n",
        "            xbeg_list.append(xbeg)\n",
        "            ybeg_list.append(ybeg)\n",
        "    data_label_filename = data_label_filename.replace('\\\\', '/')\n",
        "    data_label_filename = data_label_filename[:-4].split('/')\n",
        "    data_label_filename = data_label_filename[-1]\n",
        "    test_area = data_label_filename[5]\n",
        "    room_name = data_label_filename[7:]\n",
        "    if not os.path.exists(\"data/indoor3d_sem_seg_hdf5_data_test/raw_data3d\"):\n",
        "        os.makedirs(\"data/indoor3d_sem_seg_hdf5_data_test/raw_data3d\")\n",
        "    if not os.path.exists(\"data/indoor3d_sem_seg_hdf5_data_test/raw_data3d/Area_\"+str(test_area)):\n",
        "        os.makedirs(\n",
        "            \"data/indoor3d_sem_seg_hdf5_data_test/raw_data3d/Area_\"+str(test_area))\n",
        "    # Collect blocks\n",
        "    block_data_list = []\n",
        "    block_label_list = []\n",
        "    global raw_data_index\n",
        "    for idx in range(len(xbeg_list)):\n",
        "       xbeg = xbeg_list[idx]\n",
        "       ybeg = ybeg_list[idx]\n",
        "       xcond = (data[:, 0] <= xbeg+block_size) & (data[:, 0] >= xbeg)\n",
        "       ycond = (data[:, 1] <= ybeg+block_size) & (data[:, 1] >= ybeg)\n",
        "       cond = xcond & ycond\n",
        "       if np.sum(cond) < 100:  # discard block if there are less than 100 pts.\n",
        "           continue\n",
        "\n",
        "       block_data = data[cond, :]\n",
        "       block_label = label[cond]\n",
        "\n",
        "       # randomly subsample data\n",
        "       block_data_sampled, block_label_sampled = \\\n",
        "           sample_data_label(block_data, block_label, num_point)\n",
        "       block_data_list.append(np.expand_dims(block_data_sampled, 0))\n",
        "       block_label_list.append(np.expand_dims(block_label_sampled, 0))\n",
        "       f = open('data/indoor3d_sem_seg_hdf5_data_test/raw_data3d/Area_' +\n",
        "                str(test_area)+'/'+str(room_name)+'('+str(raw_data_index)+').txt', \"a\")\n",
        "       np.savetxt(f, block_data_sampled[:, 0:3], fmt='%s', delimiter=' ')\n",
        "       raw_data_index = raw_data_index + 1\n",
        "    return np.concatenate(block_data_list, 0), \\\n",
        "        np.concatenate(block_label_list, 0)\n",
        "\n",
        "\n",
        "def room2blocks_plus(data_label, num_point, block_size, stride,\n",
        "                     random_sample, sample_num, sample_aug):\n",
        "    \"\"\" room2block with input filename and RGB preprocessing.\n",
        "    \"\"\"\n",
        "    data = data_label[:, 0:6]\n",
        "    data[:, 3:6] /= 255.0\n",
        "    label = data_label[:, -1].astype(np.uint8)\n",
        "\n",
        "    return room2blocks(data, label, num_point, block_size, stride,\n",
        "                       random_sample, sample_num, sample_aug)\n",
        "\n",
        "\n",
        "def room2blocks_wrapper(data_label_filename, num_point, block_size=1.0, stride=1.0,\n",
        "                        random_sample=False, sample_num=None, sample_aug=1):\n",
        "    if data_label_filename[-3:] == 'txt':\n",
        "        data_label = np.loadtxt(data_label_filename)\n",
        "    elif data_label_filename[-3:] == 'npy':\n",
        "        data_label = np.load(data_label_filename)\n",
        "    else:\n",
        "        print('Unknown file type! exiting.')\n",
        "        exit()\n",
        "    return room2blocks_plus(data_label, num_point, block_size, stride,\n",
        "                            random_sample, sample_num, sample_aug)\n",
        "\n",
        "\n",
        "def room2blocks_plus_normalized(data_label_filename, data_label, num_point, block_size, stride,\n",
        "                                random_sample, sample_num, sample_aug):\n",
        "    \"\"\" room2block, with input filename and RGB preprocessing.\n",
        "        for each block centralize XYZ, add normalized XYZ as 678 channels\n",
        "    \"\"\"\n",
        "    data = data_label[:, 0:6]\n",
        "    data[:, 3:6] /= 255.0\n",
        "    label = data_label[:, -1].astype(np.uint8)\n",
        "    max_room_x = max(data[:, 0])\n",
        "    max_room_y = max(data[:, 1])\n",
        "    max_room_z = max(data[:, 2])\n",
        "    data_batch, label_batch = room2blocks(data_label_filename, data, label, num_point, block_size, stride,\n",
        "                                          random_sample, sample_num, sample_aug)\n",
        "    new_data_batch = np.zeros((data_batch.shape[0], num_point, 9))\n",
        "    for b in range(data_batch.shape[0]):\n",
        "        new_data_batch[b, :, 6] = data_batch[b, :, 0]/max_room_x\n",
        "        new_data_batch[b, :, 7] = data_batch[b, :, 1]/max_room_y\n",
        "        new_data_batch[b, :, 8] = data_batch[b, :, 2]/max_room_z\n",
        "        minx = min(data_batch[b, :, 0])\n",
        "        miny = min(data_batch[b, :, 1])\n",
        "        data_batch[b, :, 0] -= (minx+block_size/2)\n",
        "        data_batch[b, :, 1] -= (miny+block_size/2)\n",
        "    new_data_batch[:, :, 0:6] = data_batch\n",
        "    return new_data_batch, label_batch\n",
        "\n",
        "\n",
        "def room2blocks_wrapper_normalized(data_label_filename, num_point, block_size=1.0, stride=1.0,\n",
        "                                   random_sample=False, sample_num=None, sample_aug=1):\n",
        "    if data_label_filename[-3:] == 'txt':\n",
        "        data_label = np.loadtxt(data_label_filename)\n",
        "    elif data_label_filename[-3:] == 'npy':\n",
        "        data_label = np.load(data_label_filename)\n",
        "    else:\n",
        "        print('Unknown file type! exiting.')\n",
        "        exit()\n",
        "    return room2blocks_plus_normalized(data_label_filename, data_label, num_point, block_size, stride,\n",
        "                                       random_sample, sample_num, sample_aug)\n",
        "\n",
        "\n",
        "def save_h5(h5_filename, data, label, data_dtype='uint8', label_dtype='uint8'):\n",
        "    h5_fout = h5py.File(h5_filename, \"w\")\n",
        "    h5_fout.create_dataset(\n",
        "        'data', data=data,\n",
        "        compression='gzip', compression_opts=4,\n",
        "        dtype=data_dtype)\n",
        "    h5_fout.create_dataset(\n",
        "        'label', data=label,\n",
        "        compression='gzip', compression_opts=1,\n",
        "        dtype=label_dtype)\n",
        "    h5_fout.close()\n",
        "\n",
        "\n",
        "def gen_indoor3d_h5():\n",
        "    indoor3d_data_dir = 'data/stanford_indoor3d'\n",
        "    NUM_POINT = 4096\n",
        "    H5_BATCH_SIZE = 1000\n",
        "    data_dim = [NUM_POINT, 9]\n",
        "    label_dim = [NUM_POINT]\n",
        "    data_dtype = 'float32'\n",
        "    label_dtype = 'uint8'\n",
        "\n",
        "    # Set paths\n",
        "    filelist = 'metadata/all_data_label.txt'\n",
        "    data_label_files = [os.path.join(indoor3d_data_dir, line.rstrip()) for line in open(filelist)]\n",
        "    output_dir = 'data/indoor3d_sem_seg_hdf5_data_test'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "    output_filename_prefix = os.path.join(output_dir, 'ply_data_all')\n",
        "    output_room_filelist = os.path.join(output_dir, 'room_filelist.txt')\n",
        "    output_all_file = os.path.join(output_dir, 'all_files.txt')\n",
        "    fout_room = open(output_room_filelist, 'w')\n",
        "    all_file = open(output_all_file, 'w')\n",
        "\n",
        "    # --------------------------------------\n",
        "    # ----- BATCH WRITE TO HDF5 -----\n",
        "    # --------------------------------------\n",
        "    batch_data_dim = [H5_BATCH_SIZE] + data_dim\n",
        "    batch_label_dim = [H5_BATCH_SIZE] + label_dim\n",
        "    h5_batch_data = np.zeros(batch_data_dim, dtype = np.float32)\n",
        "    h5_batch_label = np.zeros(batch_label_dim, dtype = np.uint8)\n",
        "    buffer_size = 0  # state: record how many samples are currently in buffer\n",
        "    h5_index = 0 # state: the next h5 file to save\n",
        "\n",
        "    def insert_batch(data, label, last_batch=False):\n",
        "        nonlocal h5_batch_data, h5_batch_label\n",
        "        nonlocal buffer_size, h5_index\n",
        "        data_size = data.shape[0]\n",
        "        # If there is enough space, just insert\n",
        "        if buffer_size + data_size <= h5_batch_data.shape[0]:\n",
        "            h5_batch_data[buffer_size:buffer_size+data_size, ...] = data\n",
        "            h5_batch_label[buffer_size:buffer_size+data_size] = label\n",
        "            buffer_size += data_size\n",
        "        else:  # not enough space\n",
        "            capacity = h5_batch_data.shape[0] - buffer_size\n",
        "            assert (capacity >= 0)\n",
        "            if capacity > 0:\n",
        "                h5_batch_data[buffer_size:buffer_size +\n",
        "                                capacity, ...] = data[0:capacity, ...]\n",
        "                h5_batch_label[buffer_size:buffer_size +\n",
        "                                capacity, ...] = label[0:capacity, ...]\n",
        "            # Save batch data and label to h5 file, reset buffer_size\n",
        "            h5_filename = output_filename_prefix + '_' + str(h5_index) + '.h5'\n",
        "            save_h5(h5_filename, h5_batch_data,\n",
        "                                h5_batch_label, data_dtype, label_dtype)\n",
        "            print('Stored {0} with size {1}'.format(h5_filename, h5_batch_data.shape[0]))\n",
        "            h5_index += 1\n",
        "            buffer_size = 0\n",
        "            # recursive call\n",
        "            insert_batch(data[capacity:, ...], label[capacity:, ...], last_batch)\n",
        "        if last_batch and buffer_size > 0:\n",
        "            h5_filename = output_filename_prefix + '_' + str(h5_index) + '.h5'\n",
        "            save_h5(h5_filename, h5_batch_data[0:buffer_size, ...], h5_batch_label[0:buffer_size, ...], data_dtype, label_dtype)\n",
        "            print('Stored {0} with size {1}'.format(h5_filename, buffer_size))\n",
        "            h5_index += 1\n",
        "            buffer_size = 0\n",
        "        return\n",
        "\n",
        "    sample_cnt = 0\n",
        "    for i, data_label_filename in enumerate(data_label_files):\n",
        "        data, label = room2blocks_wrapper_normalized(data_label_filename, NUM_POINT, block_size=1.0, stride=1,\n",
        "                                                    random_sample=False, sample_num=None)\n",
        "        print('{0}, {1}'.format(data.shape, label.shape))\n",
        "        for _ in range(data.shape[0]):\n",
        "            fout_room.write(os.path.basename(data_label_filename)[0:-4]+'\\n')\n",
        "\n",
        "        sample_cnt += data.shape[0]\n",
        "        insert_batch(data, label, i == len(data_label_files)-1)\n",
        "\n",
        "    fout_room.close()\n",
        "    print(\"Total samples: {0}\".format(sample_cnt))\n",
        "\n",
        "    for i in range(h5_index):\n",
        "        all_file.write(os.path.join('indoor3d_sem_seg_hdf5_data_test', 'ply_data_all_') + str(i) +'.h5\\n')\n",
        "    all_file.close()\n",
        "    return\n",
        "\n",
        "\n",
        "def collect_point_label(annotation_path, out_filename, file_format='txt'):\n",
        "    \"\"\"\n",
        "    Convert original dataset files to data_label file (each line is XYZRGBL).\n",
        "    We aggregated all the points from each instance in the room.\n",
        "    Args:\n",
        "        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n",
        "        out_filename: path to save collected points and labels (each line is XYZRGBL)\n",
        "        file_format: txt or numpy, determines what file format to save.\n",
        "    Returns:\n",
        "        None\n",
        "    Note:\n",
        "        the points are shifted before save, the most negative point is now at origin.\n",
        "    \"\"\"\n",
        "    points_list = []\n",
        "    for f in glob.glob(os.path.join(annotation_path, '*.txt')):\n",
        "        cls = os.path.basename(f).split('_')[0]\n",
        "        if cls not in g_classes: # note: in some room there is 'staris' class..\n",
        "            cls = 'clutter'\n",
        "        points = np.loadtxt(f)\n",
        "        labels = np.ones((points.shape[0],1)) * g_class2label[cls]\n",
        "        points_list.append(np.concatenate([points, labels], 1)) # Nx7\n",
        "    data_label = np.concatenate(points_list, 0)\n",
        "    xyz_min = np.amin(data_label, axis=0)[0:3]\n",
        "    data_label[:, 0:3] -= xyz_min\n",
        "    if file_format=='txt':\n",
        "        fout = open(out_filename, 'w')\n",
        "        for i in range(data_label.shape[0]):\n",
        "            fout.write('%f %f %f %d %d %d %d\\n' % \\\n",
        "                          (data_label[i,0], data_label[i,1], data_label[i,2],\n",
        "                           data_label[i,3], data_label[i,4], data_label[i,5],\n",
        "                           data_label[i,6]))\n",
        "        fout.close()\n",
        "    elif file_format=='numpy':\n",
        "        np.save(out_filename, data_label)\n",
        "    else:\n",
        "        print('ERROR!! Unknown file format: %s, please use txt or numpy.' % \\\n",
        "            (file_format))\n",
        "        exit()\n",
        "\n",
        "def collect_indoor3d_data():\n",
        "    annotation_paths = [line.rstrip() for line in open('metadata/annotation_paths.txt')]\n",
        "    annotation_paths = [os.path.join('data/Stanford3dDataset_v1.2_Aligned_Version', p) for p in annotation_paths]\n",
        "    output_folder = 'data/stanford_indoor3d'\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.mkdir(output_folder)\n",
        "    for anno_path in annotation_paths:\n",
        "        elements = anno_path.split('/')\n",
        "        out_filename = elements[-3].split(\"\\\\\")[-1]+'_'+elements[-2]+'.npy' # Area_1_hallway_1.npy\n",
        "        collect_point_label(anno_path, os.path.join(output_folder, out_filename), 'numpy')\n",
        "\n",
        "\n",
        "def download_S3DIS():\n",
        "    if not os.path.exists('data/indoor3d_sem_seg_hdf5_data'):\n",
        "        with zipfile.ZipFile('data/indoor3d_sem_seg_hdf5_data.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('data/')\n",
        "    if os.path.exists('data/Stanford3dDataset_v1.2_Aligned_Version'):\n",
        "        return None\n",
        "    zippath = 'data/Stanford3dDataset_v1.2_Aligned_Version.zip'\n",
        "    with zipfile.ZipFile(zippath, \"r\") as zip_ref:\n",
        "        zip_ref.extractall('data/')\n",
        "\n",
        "\n",
        "def prepare_test_data_semseg():\n",
        "    if not os.path.exists('data/stanford_indoor3d'):\n",
        "        collect_indoor3d_data()\n",
        "    if not os.path.exists('data/indoor3d_sem_seg_hdf5_data_test'):\n",
        "        gen_indoor3d_h5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O4PH0_TMRzik"
      },
      "outputs": [],
      "source": [
        "def load_data_semseg(partition, test_area):\n",
        "    # download_S3DIS()\n",
        "    # prepare_test_data_semseg()\n",
        "    if partition == 'train':\n",
        "        data_dir = os.path.join('data/indoor3d_sem_seg_hdf5_data')\n",
        "    else:\n",
        "        data_dir = os.path.join('data/indoor3d_sem_seg_hdf5_data_test')\n",
        "    with open(os.path.join(data_dir, \"all_files.txt\")) as f:\n",
        "        all_files = [line.rstrip() for line in f]\n",
        "    with open(os.path.join(data_dir, \"room_filelist.txt\")) as f:\n",
        "        room_filelist = [line.rstrip() for line in f]\n",
        "    data_batchlist, label_batchlist = [], []\n",
        "    for f in all_files:\n",
        "        file = h5py.File(os.path.join('data/', f), 'r+')\n",
        "        data = file[\"data\"][:]\n",
        "        label = file[\"label\"][:]\n",
        "        data_batchlist.append(data)\n",
        "        label_batchlist.append(label)\n",
        "    data_batches = np.concatenate(data_batchlist, 0)\n",
        "    seg_batches = np.concatenate(label_batchlist, 0)\n",
        "    test_area_name = \"Area_\" + str(test_area)\n",
        "    train_idxs, test_idxs = [], []\n",
        "    for i, room_name in enumerate(room_filelist):\n",
        "        if test_area_name in room_name:\n",
        "            test_idxs.append(i)\n",
        "        else:\n",
        "            train_idxs.append(i)\n",
        "    if partition == 'train':\n",
        "        all_data = data_batches[train_idxs, ...]\n",
        "        all_seg = seg_batches[train_idxs, ...]\n",
        "    else:\n",
        "        all_data = data_batches[test_idxs, ...]\n",
        "        all_seg = seg_batches[test_idxs, ...]\n",
        "    return all_data, all_seg\n",
        "\n",
        "\n",
        "def load_color_semseg():\n",
        "    colors = []\n",
        "    labels = []\n",
        "    f = open(\"metadata/semseg_colors.txt\")\n",
        "    for line in json.load(f):\n",
        "        colors.append(line['color'])\n",
        "        labels.append(line['label'])\n",
        "    semseg_colors = np.array(colors)\n",
        "    semseg_colors = semseg_colors[:, [2, 1, 0]]\n",
        "    partseg_labels = np.array(labels)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    img_size = 1500\n",
        "    img = np.zeros((500, img_size, 3), dtype=\"uint8\")\n",
        "    cv2.rectangle(img, (0, 0), (img_size, 750), [255, 255, 255], thickness=-1)\n",
        "    color_size = 64\n",
        "    color_index = 0\n",
        "    label_index = 0\n",
        "    row_index = 16\n",
        "    for _ in range(0, img_size):\n",
        "        column_index = 32\n",
        "        for _ in range(0, img_size):\n",
        "            color = semseg_colors[color_index]\n",
        "            label = partseg_labels[label_index]\n",
        "            length = len(str(label))\n",
        "            cv2.rectangle(img, (column_index, row_index), (column_index + color_size, row_index + color_size),\n",
        "                          color=(int(color[0]), int(color[1]), int(color[2])), thickness=-1)\n",
        "            img = cv2.putText(img, label, (column_index + int(color_size * 1.15), row_index + int(color_size / 2)),\n",
        "                              font,\n",
        "                              0.7, (0, 0, 0), 2)\n",
        "            column_index = column_index + 200\n",
        "            color_index = color_index + 1\n",
        "            label_index = label_index + 1\n",
        "            if color_index >= 13:\n",
        "                cv2.imwrite(\"metadata/semseg_colors.png\",\n",
        "                            img, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n",
        "                return np.array(colors)\n",
        "            elif (column_index >= 1280):\n",
        "                break\n",
        "        row_index = row_index + int(color_size * 1.3)\n",
        "        if (row_index >= img_size):\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRMUCIzRzil"
      },
      "source": [
        "##### Main class for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ocdDe7C1Rzil"
      },
      "outputs": [],
      "source": [
        "class S3DIS(Dataset):\n",
        "    def __init__(self, num_points=4096, partition='train', test_area='1'):\n",
        "        self.data, self.seg = load_data_semseg(partition, test_area)\n",
        "        self.num_points = num_points\n",
        "        self.partition = partition\n",
        "        self.semseg_colors = load_color_semseg()\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        pointcloud = self.data[item][:self.num_points]\n",
        "        seg = self.seg[item][:self.num_points]\n",
        "        if self.partition == 'train':\n",
        "            indices = list(range(pointcloud.shape[0]))\n",
        "            np.random.shuffle(indices)\n",
        "            pointcloud = pointcloud[indices]\n",
        "            seg = seg[indices]\n",
        "        seg = torch.LongTensor(seg)\n",
        "        return pointcloud, seg\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MJ_kw7kRzil"
      },
      "source": [
        "### 3. Train test sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzxSyU5hRzil"
      },
      "source": [
        "##### Arguments to be initialized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "80Y0ElzARzil"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    \"exp_name\": \"exp\",\n",
        "    'model': 'dgcnn',\n",
        "    'dataset': 'S3DIS',\n",
        "    'test_area': 6,\n",
        "    'batch_size': 64,\n",
        "    'test_batch_size': 16,\n",
        "    'epochs': 30,\n",
        "    'use_sgd': True,\n",
        "    'lr': 0.001,\n",
        "    'momentum': 0.9,\n",
        "    'scheduler': 'cos',\n",
        "    'cuda': True,\n",
        "    'seed': 2024,\n",
        "    'eval': True,\n",
        "    'num_points': 4096,\n",
        "    'dropout': 0.5,\n",
        "    'embedded_dims': 1024,\n",
        "    'k': 20,\n",
        "    'model_root': '',\n",
        "    'visu': '',\n",
        "    'visu_format': 'ply'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUqJVEshRzil"
      },
      "source": [
        "##### Init function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MkhEHBttRzim"
      },
      "outputs": [],
      "source": [
        "def init_state():\n",
        "    if not os.path.exists('outputs'):\n",
        "        os.makedirs('outputs')\n",
        "    if not os.path.exists('outputs/'+args['exp_name']):\n",
        "        os.makedirs('outputs/'+args['exp_name'])\n",
        "    if not os.path.exists('outputs/'+args['exp_name']+'/'+'models'):\n",
        "        os.makedirs('outputs/'+args['exp_name']+'/'+'models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC7l78vCRzim"
      },
      "source": [
        "##### Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kNi8aMweRzim"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    train_loader = DataLoader(S3DIS(partition='train', num_points=args['num_points'], test_area=args['test_area']),\n",
        "                              num_workers=8, batch_size=args['batch_size'], shuffle=True, drop_last=True)\n",
        "    print(\"Loaded Train data\")\n",
        "    test_loader = DataLoader(S3DIS(partition='test', num_points=args['num_points'], test_area=args['test_area']),\n",
        "                             num_workers=8, batch_size=args['test_batch_size'], shuffle=True, drop_last=False)\n",
        "    print(\"Loaded Test data\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
        "\n",
        "    if args['model'] == 'dgcnn':\n",
        "        model = DGCNN(args).to(device)\n",
        "    else:\n",
        "        raise Exception(\"Not implemented\")\n",
        "    print(str(model))\n",
        "    model = nn.DataParallel(model)\n",
        "    print(\"We are using \", torch.cuda.device_count(), \" GPUs!\")\n",
        "\n",
        "    if args['use_sgd']:\n",
        "        print(\"Use SGD\")\n",
        "        opt = optim.SGD(model.parameters(),\n",
        "                        lr=args['lr']*100,\n",
        "                        momentum=args['momentum'],\n",
        "                        weight_decay=1e-4)\n",
        "    else:\n",
        "        print(\"Use Adam\")\n",
        "        opt = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=1e-4)\n",
        "\n",
        "    if args['scheduler'] == 'cos':\n",
        "        scheduler = CosineAnnealingLR(opt, args['epochs'], eta_min=1e-3)\n",
        "    elif args['scheduler'] == 'step':\n",
        "        scheduler = StepLR(opt, 20, 0.5, args['epochs'])\n",
        "\n",
        "    criterion = cal_loss\n",
        "\n",
        "    best_test_iou = 0\n",
        "\n",
        "    for epoch in range(args['epochs']):\n",
        "        # Train\n",
        "        train_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.train()\n",
        "        train_true_cls = []\n",
        "        train_pred_cls = []\n",
        "        train_true_seg = []\n",
        "        train_pred_seg = []\n",
        "        train_label_seg = []\n",
        "        for data, seg in train_loader:\n",
        "            data, seg = data.to(device), seg.to(device)\n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            opt.zero_grad()\n",
        "            seg_pred = model(data)\n",
        "            seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n",
        "            loss = criterion(seg_pred.view(-1, 13), seg.view(-1, 1).squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            pred = seg_pred.max(dim=2)[1]            # (batch_size, num_points)\n",
        "            count += batch_size\n",
        "            train_loss += loss.item() * batch_size\n",
        "            seg_np = seg.cpu().numpy()                  # (batch_size, num_points)\n",
        "            pred_np = pred.detach().cpu().numpy()       # (batch_size, num_points)\n",
        "            train_true_cls.append(seg_np.reshape(-1))   # (batch_size * num_points)\n",
        "            train_pred_cls.append(pred_np.reshape(-1))  # (batch_size * num_points)\n",
        "            train_true_seg.append(seg_np)\n",
        "            train_pred_seg.append(pred_np)\n",
        "        if args['scheduler'] == 'cos':\n",
        "            scheduler.step()\n",
        "        elif args['scheduler'] == 'step':\n",
        "            if opt.param_groups[0]['lr'] > 1e-5:\n",
        "                scheduler.step()\n",
        "            if opt.param_groups[0]['lr'] < 1e-5:\n",
        "                for param_group in opt.param_groups:\n",
        "                    param_group['lr'] = 1e-5\n",
        "        train_true_cls = np.concatenate(train_true_cls)\n",
        "        train_pred_cls = np.concatenate(train_pred_cls)\n",
        "        train_acc = metrics.accuracy_score(train_true_cls, train_pred_cls)\n",
        "        avg_per_class_acc = metrics.balanced_accuracy_score(train_true_cls, train_pred_cls)\n",
        "        train_true_seg = np.concatenate(train_true_seg, axis=0)\n",
        "        train_pred_seg = np.concatenate(train_pred_seg, axis=0)\n",
        "        train_ious = calculate_semantic_mIoU(train_pred_seg, train_true_seg)\n",
        "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f, train iou: %.6f' % (epoch,\n",
        "                                                                                                  train_loss*1.0/count,\n",
        "                                                                                                  train_acc,\n",
        "                                                                                                  avg_per_class_acc,\n",
        "                                                                                                  np.mean(train_ious))\n",
        "        print(outstr)\n",
        "\n",
        "        # Eval\n",
        "        test_loss = 0.0\n",
        "        count = 0.0\n",
        "        model.eval()\n",
        "        test_true_cls = []\n",
        "        test_pred_cls = []\n",
        "        test_true_seg = []\n",
        "        test_pred_seg = []\n",
        "        for data, seg in test_loader:\n",
        "            data, seg = data.to(device), seg.to(device)\n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            seg_pred = model(data)\n",
        "            seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n",
        "            loss = criterion(seg_pred.view(-1, 13), seg.view(-1, 1).squeeze())\n",
        "            pred = seg_pred.max(dim=2)[1]\n",
        "            count += batch_size\n",
        "            test_loss += loss.item() * batch_size\n",
        "            seg_np = seg.cpu().numpy()\n",
        "            pred_np = pred.detach().cpu().numpy()\n",
        "            test_true_cls.append(seg_np.reshape(-1))\n",
        "            test_pred_cls.append(pred_np.reshape(-1))\n",
        "            test_true_seg.append(seg_np)\n",
        "            test_pred_seg.append(pred_np)\n",
        "        test_true_cls = np.concatenate(test_true_cls)\n",
        "        test_pred_cls = np.concatenate(test_pred_cls)\n",
        "        test_acc = metrics.accuracy_score(test_true_cls, test_pred_cls)\n",
        "        avg_per_class_acc = metrics.balanced_accuracy_score(\n",
        "            test_true_cls, test_pred_cls)\n",
        "        test_true_seg = np.concatenate(test_true_seg, axis=0)\n",
        "        test_pred_seg = np.concatenate(test_pred_seg, axis=0)\n",
        "        test_ious = calculate_semantic_mIoU(test_pred_seg, test_true_seg)\n",
        "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f, test iou: %.6f' % (epoch,\n",
        "                                                                                              test_loss*1.0/count,\n",
        "                                                                                              test_acc,\n",
        "                                                                                              avg_per_class_acc,\n",
        "                                                                                              np.mean(test_ious))\n",
        "        print(outstr)\n",
        "        if np.mean(test_ious) >= best_test_iou:\n",
        "            best_test_iou = np.mean(test_ious)\n",
        "            torch.save(model.state_dict(), 'outputs/%s/models/model_%s.t7' %\n",
        "                       (args['exp_name'], args['test_area']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PzXJlLMRzim"
      },
      "source": [
        "##### Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4juWLmcBRzim"
      },
      "outputs": [],
      "source": [
        "def test(args, io):\n",
        "    all_true_cls = []\n",
        "    all_pred_cls = []\n",
        "    all_true_seg = []\n",
        "    all_pred_seg = []\n",
        "    for test_area in range(1, 7):\n",
        "        visual_file_index = 0\n",
        "        test_area = str(test_area)\n",
        "        if os.path.exists(\"data/indoor3d_sem_seg_hdf5_data_test/room_filelist.txt\"):\n",
        "            with open(\"data/indoor3d_sem_seg_hdf5_data_test/room_filelist.txt\") as f:\n",
        "                for line in f:\n",
        "                    if (line[5]) == test_area:\n",
        "                        break\n",
        "                    visual_file_index = visual_file_index + 1\n",
        "        if (args.test_area == 'all') or (test_area == args.test_area):\n",
        "            test_loader = DataLoader(S3DIS(partition='test', num_points=args.num_points, test_area=test_area),\n",
        "                                     batch_size=args.test_batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "            device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "            # Try to load models\n",
        "            semseg_colors = test_loader.dataset.semseg_colors\n",
        "            if args.model == 'dgcnn':\n",
        "                model = DGCNN(args).to(device)\n",
        "            else:\n",
        "                raise Exception(\"Not implemented\")\n",
        "\n",
        "            model = nn.DataParallel(model)\n",
        "            model.load_state_dict(torch.load(os.path.join(\n",
        "                args.model_root, 'model_%s.t7' % test_area)))\n",
        "            model = model.eval()\n",
        "            test_acc = 0.0\n",
        "            count = 0.0\n",
        "            test_true_cls = []\n",
        "            test_pred_cls = []\n",
        "            test_true_seg = []\n",
        "            test_pred_seg = []\n",
        "            for data, seg in test_loader:\n",
        "                data, seg = data.to(device), seg.to(device)\n",
        "                data = data.permute(0, 2, 1)\n",
        "                batch_size = data.size()[0]\n",
        "                seg_pred = model(data)\n",
        "                seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n",
        "                pred = seg_pred.max(dim=2)[1]\n",
        "                seg_np = seg.cpu().numpy()\n",
        "                pred_np = pred.detach().cpu().numpy()\n",
        "                test_true_cls.append(seg_np.reshape(-1))\n",
        "                test_pred_cls.append(pred_np.reshape(-1))\n",
        "                test_true_seg.append(seg_np)\n",
        "                test_pred_seg.append(pred_np)\n",
        "                # visiualization\n",
        "                visualization(args.visu, args.visu_format, args.test_area,\n",
        "                              data, seg, pred, visual_file_index, semseg_colors)\n",
        "                visual_file_index = visual_file_index + data.shape[0]\n",
        "            if visual_warning and args.visu != '':\n",
        "                print(\n",
        "                    'Visualization Failed: You can only choose a room to visualize within the scope of the test area')\n",
        "            test_true_cls = np.concatenate(test_true_cls)\n",
        "            test_pred_cls = np.concatenate(test_pred_cls)\n",
        "            test_acc = metrics.accuracy_score(test_true_cls, test_pred_cls)\n",
        "            avg_per_class_acc = metrics.balanced_accuracy_score(\n",
        "                test_true_cls, test_pred_cls)\n",
        "            test_true_seg = np.concatenate(test_true_seg, axis=0)\n",
        "            test_pred_seg = np.concatenate(test_pred_seg, axis=0)\n",
        "            test_ious = calculate_semantic_mIoU(test_pred_seg, test_true_seg)\n",
        "            outstr = 'Test :: test area: %s, test acc: %.6f, test avg acc: %.6f, test iou: %.6f' % (test_area,\n",
        "                                                                                                    test_acc,\n",
        "                                                                                                    avg_per_class_acc,\n",
        "                                                                                                    np.mean(test_ious))\n",
        "            io.cprint(outstr)\n",
        "            all_true_cls.append(test_true_cls)\n",
        "            all_pred_cls.append(test_pred_cls)\n",
        "            all_true_seg.append(test_true_seg)\n",
        "            all_pred_seg.append(test_pred_seg)\n",
        "\n",
        "    if args.test_area == 'all':\n",
        "        all_true_cls = np.concatenate(all_true_cls)\n",
        "        all_pred_cls = np.concatenate(all_pred_cls)\n",
        "        all_acc = metrics.accuracy_score(all_true_cls, all_pred_cls)\n",
        "        avg_per_class_acc = metrics.balanced_accuracy_score(\n",
        "            all_true_cls, all_pred_cls)\n",
        "        all_true_seg = np.concatenate(all_true_seg, axis=0)\n",
        "        all_pred_seg = np.concatenate(all_pred_seg, axis=0)\n",
        "        all_ious = calculate_semantic_mIoU(all_pred_seg, all_true_seg)\n",
        "        outstr = 'Overall Test :: test acc: %.6f, test avg acc: %.6f, test iou: %.6f' % (all_acc,\n",
        "                                                                                         avg_per_class_acc,\n",
        "                                                                                         np.mean(all_ious))\n",
        "        io.cprint(outstr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgQBNUXVRzin"
      },
      "source": [
        "##### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "twurmMaiRzin",
        "outputId": "82c6d472-acdf-47e6-d392-79458d5ad508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Train data\n",
            "Loaded Test data\n",
            "DGCNN(\n",
            "  (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batch_norm_8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_1): Sequential(\n",
            "    (0): Conv2d(18, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_2): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_3): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_4): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_5): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_6): Sequential(\n",
            "    (0): Conv1d(192, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_7): Sequential(\n",
            "    (0): Conv1d(1216, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv_8): Sequential(\n",
            "    (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (conv_9): Conv1d(256, 13, kernel_size=(1,), stride=(1,), bias=False)\n",
            ")\n",
            "We are using  1  GPUs!\n",
            "Use SGD\n",
            "Train 0, loss: 0.733227, train acc: 0.760177, train avg acc: 0.448236, train iou: 0.350886\n",
            "Test 0, loss: 0.931659, test acc: 0.689458, test avg acc: 0.480599, test iou: 0.350019\n",
            "Train 1, loss: 0.501369, train acc: 0.828475, train avg acc: 0.580543, train iou: 0.473147\n",
            "Test 1, loss: 0.609933, test acc: 0.791612, test avg acc: 0.610221, test iou: 0.485203\n",
            "Train 2, loss: 0.419833, train acc: 0.855045, train avg acc: 0.657565, train iou: 0.548066\n",
            "Test 2, loss: 0.731152, test acc: 0.762921, test avg acc: 0.597964, test iou: 0.487789\n",
            "Train 3, loss: 0.373014, train acc: 0.870559, train avg acc: 0.707327, train iou: 0.596902\n",
            "Test 3, loss: 1.214095, test acc: 0.627604, test avg acc: 0.473301, test iou: 0.368402\n",
            "Train 4, loss: 0.344348, train acc: 0.880252, train avg acc: 0.739146, train iou: 0.628810\n",
            "Test 4, loss: 0.868896, test acc: 0.741876, test avg acc: 0.645315, test iou: 0.479234\n",
            "Train 5, loss: 0.314679, train acc: 0.890253, train avg acc: 0.763890, train iou: 0.657592\n",
            "Test 5, loss: 0.654188, test acc: 0.786886, test avg acc: 0.689497, test iou: 0.537807\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-08e2759f6a14>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-b498f96a28f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mseg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m            \u001b[0;31m# (batch_size, num_points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "init_state()\n",
        "train(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
